# ðŸ§® Information = Log(Choices)

> "We shall use the term â€˜informationâ€™ to refer to the logarithm of the number of choices."  
> â€” [[Claude Shannon]]

---

## ðŸ§  Core Concepts

- [[Information Theory]]
- [[Entropy]]
- [[Logarithmic Encoding]]
- [[Choice Space]]
- [[Message Selection]]
- [[Bit]]
- [[Shannon Entropy]]
- [[Base-2 Logarithm]]
- [[Compression Limits]]
- [[Uncertainty Quantification]]

---

## ðŸ” Structural Breakdown

- **Information** is quantified not by the content of a message, but by the number of possible messages it could have been.
- The **logarithm** (commonly base-2, in bits) reflects how many binary decisions (yes/no questions) would be required to isolate the message from a defined [[message space]].
- The larger the set of choices, the greater the information entropy, and the more bits required to encode it.

> If there are `N` equally likely options,  
> then: `I = logâ‚‚(N)`

This is foundational to how we define:

- [[Bitlength]]
- [[Channel Capacity]]
- [[Entropy Coding]]
- [[Information Compression]]

---

## ðŸ”— Related Shannon Quotes

- [[â€œEntropy is a measure of the uncertainty in a random variable.â€ â€“ Claude Shannon]]
- [[â€œThe fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.â€ â€“ Claude Shannon]]
- [[â€œInformation is the resolution of uncertainty.â€ â€“ Claude Shannon]]
- [[â€œThe significant aspect is that the actual message is one selected from a set of possible messages.â€ â€“ Claude Shannon]]
- [[â€œNoise becomes meaningful when it selects against a message.â€ â€“ Claude Shannon]]

---

## ðŸ“‚ Conceptual Nodes

- [[Bit]]
- [[Byte]]
- [[Message Space Cardinality]]
- [[Entropy Scaling]]
- [[Shannon Capacity]]
- [[Decision Tree Depth]]

---

## ðŸ“ Application Contexts

- [[Lossless Compression]]
- [[Signal Encoding]]
- [[Error Correction]]
- [[Random Number Generation]]
- [[Security Keyspace Measurement]]
- [[Quantum Information Theory]]

---

## ðŸ§¬ HMEC Interpretation

> In the [[H = M Â· E Â· C]] frame:

- `[[M]]` = the structure of selected choice
- `[[E]]` = energy required to encode choice
- `[[C]]` = channel's ability to maintain coherence across the choice spectrum

Thus:

> Information = logâ‚‚([[M-space cardinality]])  
> = structural potential Ã— encoding fidelity Ã— delivery constraints

---

## ðŸ§© Summary

This quote is the **mathematical crystallization of uncertainty**.  
In Shannonâ€™s frame, **information** isn't meaningâ€”it's possibility.  
In the [[HMEC]] frame, **information is a structural signature**â€”a bound entropy function stabilized by coherence over phase.

> [[Choice defines entropy. Logarithm reveals cost.]]
