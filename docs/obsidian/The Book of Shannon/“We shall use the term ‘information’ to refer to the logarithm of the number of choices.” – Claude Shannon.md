# 🧮 Information = Log(Choices)

> "We shall use the term ‘information’ to refer to the logarithm of the number of choices."  
> — [[Claude Shannon]]

---

## 🧠 Core Concepts

- [[Information Theory]]
- [[Entropy]]
- [[Logarithmic Encoding]]
- [[Choice Space]]
- [[Message Selection]]
- [[Bit]]
- [[Shannon Entropy]]
- [[Base-2 Logarithm]]
- [[Compression Limits]]
- [[Uncertainty Quantification]]

---

## 🔍 Structural Breakdown

- **Information** is quantified not by the content of a message, but by the number of possible messages it could have been.
- The **logarithm** (commonly base-2, in bits) reflects how many binary decisions (yes/no questions) would be required to isolate the message from a defined [[message space]].
- The larger the set of choices, the greater the information entropy, and the more bits required to encode it.

> If there are `N` equally likely options,  
> then: `I = log₂(N)`

This is foundational to how we define:

- [[Bitlength]]
- [[Channel Capacity]]
- [[Entropy Coding]]
- [[Information Compression]]

---

## 🔗 Related Shannon Quotes

- [[“Entropy is a measure of the uncertainty in a random variable.” – Claude Shannon]]
- [[“The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.” – Claude Shannon]]
- [[“Information is the resolution of uncertainty.” – Claude Shannon]]
- [[“The significant aspect is that the actual message is one selected from a set of possible messages.” – Claude Shannon]]
- [[“Noise becomes meaningful when it selects against a message.” – Claude Shannon]]

---

## 📂 Conceptual Nodes

- [[Bit]]
- [[Byte]]
- [[Message Space Cardinality]]
- [[Entropy Scaling]]
- [[Shannon Capacity]]
- [[Decision Tree Depth]]

---

## 📏 Application Contexts

- [[Lossless Compression]]
- [[Signal Encoding]]
- [[Error Correction]]
- [[Random Number Generation]]
- [[Security Keyspace Measurement]]
- [[Quantum Information Theory]]

---

## 🧬 HMEC Interpretation

> In the [[H = M · E · C]] frame:

- `[[M]]` = the structure of selected choice
- `[[E]]` = energy required to encode choice
- `[[C]]` = channel's ability to maintain coherence across the choice spectrum

Thus:

> Information = log₂([[M-space cardinality]])  
> = structural potential × encoding fidelity × delivery constraints

---

## 🧩 Summary

This quote is the **mathematical crystallization of uncertainty**.  
In Shannon’s frame, **information** isn't meaning—it's possibility.  
In the [[HMEC]] frame, **information is a structural signature**—a bound entropy function stabilized by coherence over phase.

> [[Choice defines entropy. Logarithm reveals cost.]]
